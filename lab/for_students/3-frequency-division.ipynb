{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6884f06c-f78b-405b-860f-3aa47a48f670",
   "metadata": {},
   "source": [
    "# Part 3: Frequency Multiplexing: Transmitting Multiple Voice Signals \n",
    "\n",
    "**Objectives:** \n",
    "* Show how signals can be moved to different frequencies and combined for transmission\n",
    "* Show how the combined signals can be separated in the frequency domain\n",
    "* Explain how this is used in wireless communications\n",
    "\n",
    "**Required Materials**\n",
    "* Computer speakers\n",
    "* Python audio libraries noted below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec37a6b-ec8f-46b0-8618-e3e95e20561f",
   "metadata": {},
   "source": [
    "## Loading the Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18543776-6399-4e83-a832-fb620193a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the minimum sound libraries required. \n",
    "\n",
    "import scipy.io.wavfile as wavfile\n",
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff19bc3-4ebf-48d8-849a-7af150556ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for visualizing the sound recordings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91548c4a-fc18-48f7-845c-12e2e061bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper libraries\n",
    "\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a775346-2ce0-457e-97dd-b143a526168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this block loads some elements for adding interactive \n",
    "# widgets to the code\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import IPython.display as display\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3926d55-68dd-468a-ab15-b03528a695d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook uses a couple of new function from SciPy\n",
    "\n",
    "from scipy.signal import periodogram, butter, sosfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71975c0-a5c3-4222-bc1a-3575f8e7917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook uses a couple of new functions from NumPy to transform a signal into a frequency\n",
    "# representation\n",
    "\n",
    "import numpy.fft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400832ec-b224-4838-8f43-6820a69afcd1",
   "metadata": {},
   "source": [
    "## Frequency Multiplexing: Part 1 -- Low Pass Filtering\n",
    "\n",
    "*Multiplexing* is a process by which multiple communication signals are combined to use a single medium. Depending on the nature of the signals, they may be multiplexed across time, frequency, or through other approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438d2e7d-463a-4609-b849-02afd73b9804",
   "metadata": {},
   "source": [
    "In this section, we apply frequency-domain techniques to multiplex voice signals. In this approach, different voice signals are moved to different parts of the frequency range. When the different signals correspond to different users, this approach is called *frequency-division multiple access (FDMA)*. FDMA was used in analog cellular communication systems, but the audio signals were multiplexed by placing the signals in different radio frequency (RF) bands.\n",
    "\n",
    "In this lab, we will place the signals in different audio bands, so that we can hear the effects of multiplexing. To do this, we will need to limit the audio signals bandwidth so that we can combine multiple audio signals within the same audio frequency  band. We start by demonstrating that voice signals can be truncated to a much lower frequency band than the human ear can hear and then use that fact to limit the frequency range that will be preserved.\n",
    "\n",
    "First, load two voice signals to work with, by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff341033-6d89-4f57-9879-94f2e0b25553",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate1, voice1 = wavfile.read('testing.wav')\n",
    "rate2, voice2 = wavfile.read('quickbrownfox.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a3fb5f-2f5d-4fb3-8d95-39175d2a2a07",
   "metadata": {},
   "source": [
    "Now, let's listen to each signal and use the periodogram to plot its power by frequency band:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ced0c-ae03-4f01-a8a5-5f26ab7844f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(voice1, rate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c4c7f-e30e-4689-bccb-dccad0e2f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, P1 = periodogram(voice1, rate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f7554-430b-4976-b392-e0d25cf3038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f1,P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ac46c-b983-4c68-9018-ec3456414892",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(voice2, rate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9f2ba-9d5f-417d-9a3d-7808551cce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2, P2 = periodogram(voice2, rate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e28aa4-7318-476e-87fd-ae4a13320f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f2, P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ab494-0e2f-429b-967f-522d53251da2",
   "metadata": {},
   "source": [
    "Note that most of the power in these signals is contained in the lower frequencies of the human hearing range. We can use *low pass filtering* to remove signal components that are at higher frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef1da0-0351-48df-b975-d391b614fcb1",
   "metadata": {},
   "source": [
    "In this lab, we will use a very simple approach to remove all the high-frequency content. We will transform the signal into the frequency domain using something called the fast Fourier transform (FFT). You don't need to know the details of how the FFT works -- you just need to know that it captures **all** the information about the signal, but in a frequency-domain version.\n",
    "\n",
    "If we plot absolute values of the FFT (with a shift to center it), the output of the FFT looks like a two-sided version of the periodogram output. (Don't worry about what negative frequency means for this function -- just know that the negative part will always be symmetric with the positive part for real signals.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0cd80-3149-4f89-99df-cc3a9d3df55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft1 = numpy.fft.fft(voice1)\n",
    "freqs1 = numpy.fft.fftfreq(len(voice1), 1/rate1)\n",
    "plt.plot(freqs1, abs((fft1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbf75c-f4e3-4d26-ba6e-df96a8e6a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft2 = numpy.fft.fft(voice2)\n",
    "freqs2 = numpy.fft.fftfreq(len(voice1), 1/rate2)\n",
    "plt.plot(freqs2, abs((fft2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38134ddc-d4a2-4504-8058-fff37163b6c7",
   "metadata": {},
   "source": [
    "We can remove the high-frequency information by zeroing it out. The use landline telephone system, which is also called Plain Old Telephone Service (POTS), uses a cutoff of 3.4 KHz for the audio signals, so let's do the same. The parameter `cutoff` below is set to 3400. After completing the activities in this notebook, feel free to come back and change the value of `cutoff` to see the effects of different choices of this parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acdafac-1259-4b40-b8e0-39e001a4e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 3400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd732c2-ce3e-4697-a88b-8955c6ebc1be",
   "metadata": {},
   "source": [
    "The following will plot a line showing which frequencies fall below `cutoff` with the `voice1` signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c98dc-24c4-437e-99c6-b9914784a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft1 = numpy.fft.fft(voice1)\n",
    "freqs1 = numpy.fft.fftfreq(len(voice1), 1/rate1)\n",
    "plt.plot(freqs1, abs((fft1)))\n",
    "plt.plot(freqs1, 500000 * (abs(freqs1) <= cutoff) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2a56db-aea6-4fd5-8180-12928dcf3514",
   "metadata": {},
   "source": [
    "Now run the following blocks to zero out all frequencies  over `cutoff`. This block will also remove the value at 0 Hz because that is an offset that does not affect the sound and makes interpreting the signals easier later.  Listen to the signals before and after the high frequencies are removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06987c64-0192-4025-880b-4068a38e75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered1 = fft1.copy()\n",
    "filtered1[np.where(abs(freqs1) > cutoff)] = 0\n",
    "filtered1[0]=0\n",
    "plt.plot(freqs1, abs(filtered1) )\n",
    "plt.plot(freqs1, 500000 * (abs(freqs1) <= cutoff) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4077ec-871a-43bb-aca3-6f1b2c42bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.real(np.fft.ifft(filtered1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea594ceb-f0fe-47aa-b6c3-6a811b8c971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original \"testing\" voice signal:')\n",
    "sd.play(voice1)\n",
    "time.sleep(3)\n",
    "print('Filtered \"testing\" signal:')\n",
    "voice_filtered1 = np.real( np.fft.ifft(filtered1)/16384 )\n",
    "sd.play(voice_filtered1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c03960-7486-4f9e-be64-9256735de2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered2 = fft2.copy()\n",
    "filtered2[np.where(abs(freqs2) > cutoff)] = 0\n",
    "plt.plot(freqs2, abs(filtered2) )\n",
    "plt.plot(freqs2, 500000 * (abs(freqs2) <= cutoff) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601514f-d168-4b1b-80c9-ec0ab1be0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original \"quick brown fox\" voice signal:')\n",
    "sd.play(voice2)\n",
    "time.sleep(3)\n",
    "print('Filtered \"quick brown fox\" signal:')\n",
    "voice_filtered2 = np.real( np.fft.ifft(filtered2)/16384 )\n",
    "sd.play(voice_filtered2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd89b3-01ab-423c-83a0-2ce9218b525e",
   "metadata": {},
   "source": [
    "**Question 3.1: How do the filtered signals sound in comparison to the original signals?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df678fde-dfe7-42fc-86ad-d7ecb820d9fd",
   "metadata": {},
   "source": [
    "## Frequency Multiplexing: Part 2 -- Frequency Shifting and Combining\n",
    "\n",
    "To multiplex two low-pass signals (i.e., signals with no frequency content above some cutoff frequency) into a single signal in the audio frequencies, we can leave on signal at frequencies < cutoff and shift the other signal so that all of its audio content is above the cutoff. \n",
    "\n",
    "POTS uses a separation of 4 kHz, so that is what we will do here as well. Let's leave the `filtered_voice1` signal alone  -- we say that it is at **baseband**, which means that its frequency content starts at 0 Hz. \n",
    "\n",
    "Now let's create a shifted version of the `filtered_voice2` signal by shifting its fft content. The approach wee will use it to move all of the frequency components (positive and negative) up to 8 kHz. 8kHz is chosen because the shifted signal will now go from 8-3.4 = 4.6 kHz to 8+3.4= 11.4 kHz,\n",
    "thus not interfering with the original signal. \n",
    "\n",
    "When we shift the signal up to 8 kHz, we also have to shift its mirror image down by 8 kHz to preserve the symmetry.\n",
    "\n",
    "**NOTE 1:** You might wonder why we don't just shift the right half of the original signal up by 4 kHz and the left half of the original signal down by 4 kHz. That would work as well, **but the new signal would not sound anything like the original voice signal.**\n",
    "\n",
    "**NOTE 2:** This is *frequency shifting*, not *pitch shifting*. In pitch shifting, the *multiplicative* relation among tones is preserved. By adding a frequency shift, the multiplicative relation between tones is broken, thereby distorting the original pitch relations.\n",
    "\n",
    "To perform the shift, we need to know the index of 8 kHz in the original frequency data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f17c54-a3cc-436d-bda2-999bad230003",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(freqs2>8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a46f83-e383-4618-af35-04154f0e4420",
   "metadata": {},
   "source": [
    "A 8 kHz shift corresponds to a shifting the values in the FFT block by 23963. I will set the parameter `freq_shift` to 11982. When you complete this notebook, you can come back and experiment with  different values of `freq_shift`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8759245-824c-478e-a1f1-6ab784388565",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_shift=23963"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a030fcda-d521-4889-81a9-c6d0bb17aeda",
   "metadata": {},
   "source": [
    "The following code will do the necessary shifting. Again, don't worry about the details -- the data in the FFT blocks is arranged in an usual way, and this code will do the necessary shifting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00839260-8c88-4ac8-a3fb-2cd77abe5c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted2 = np.roll(filtered2, freq_shift)\n",
    "shifted2+= np.roll(filtered2[::], -freq_shift-1 )\n",
    "\n",
    "plt.plot(freqs2, abs(shifted2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e53ac-222b-45ad-bcb3-640b303be7c4",
   "metadata": {},
   "source": [
    "**Note:** In practice, frequency shifting is not done this way. In fact, frequency shifting is even easier because it just involves multiplying by a sinusoid (a pure tone) and then filtering out low frequency content. The method used here is to keep everything in the frequency domain so that the shifting is more easily understood.\n",
    "\n",
    "Listen to the frequency-shifted (**not pitch-shifted**) signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf850ca-aaa6-4fc0-9778-3aa7cad5a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "voice_shifted2=np.real(np.fft.ifft(shifted2))/500\n",
    "\n",
    "sd.play(voice_shifted2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d1429-72a9-40fe-aadc-ec3f555e3144",
   "metadata": {},
   "source": [
    "**Question 3.2: Describe how the frequency-shifted signal sounds compared to the original signal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ca8cf-01ea-4f4c-96bd-dc4c5704fa72",
   "metadata": {},
   "source": [
    "The code below plots the filtered signal and its frequency-shifted counterpart. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18a715-2bda-4fca-99b4-08fb4e2f066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=10000\n",
    "length=500\n",
    "\n",
    "plt.plot(voice_filtered2[start: start+length])\n",
    "plt.title('Filtered signal')\n",
    "\n",
    "plt.figure()\n",
    "voice_shifted2=np.real(np.fft.ifft(shifted2))\n",
    "plt.plot(voice_shifted2[start: start+length])\n",
    "plt.title('Frequency-shifted version of filtered signal');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965eb5d6-56df-429a-ab74-8171269579d1",
   "metadata": {},
   "source": [
    "\n",
    "**Question 3.3: How does the plot of the frequency-shifted signal compare to the plot of the original signal?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ed3eec-947f-4887-b2c1-a1b147463582",
   "metadata": {},
   "source": [
    "Now the filtered `voice1` signal and the `shifted2`, which is the filtered and frequency-shifted version of the `voice2` signal occupy completely different frequency bands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9a189-9ac1-4446-a3e8-508fbf4fbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freqs1, abs(filtered1) )\n",
    "plt.plot(freqs2, abs(shifted2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06d89e-dc8a-4d97-b4ba-0af99b49e0f5",
   "metadata": {},
   "source": [
    "We can combine the two signals into one through simple addition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209add5e-d34e-4fc1-a9d0-59e20a2c08bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplexed = filtered1 + shifted2\n",
    "plt.plot(freqs1, abs(multiplexed) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef157e-e7b1-430b-a581-0f4f566d0a96",
   "metadata": {},
   "source": [
    "**Activity 3.4:  Listen to the combined signal. Can you hear the different messages? Propose an explanation for what you have heard.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722c07d-dc63-48c7-be2e-740e64085e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_multiplexed = np.real( numpy.fft.ifft(multiplexed)/4096 )\n",
    "sd.play(voice_multiplexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8409df7a-6600-48bf-9a1b-6447c61ebd21",
   "metadata": {},
   "source": [
    "## Frequency Demultiplexing\n",
    "\n",
    "Demultiplexing is the process of recovering the individual signals from a multiplexed signal. We will use FFT again, so that we can see the process in the frequency domain. However, in practice, these techniques are usually implemented using different signal processing techniques.\n",
    "\n",
    "To recover the signal at baseband (the \"testing\" signal), we just need to zero out all the higher frequency content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c2d5e-9f39-4809-abff-c71bdeddc2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_freqs = np.where(abs(freqs1)>4000)\n",
    "demultiplexed1 = multiplexed.copy()\n",
    "demultiplexed1[high_freqs]=0\n",
    "plt.plot(freqs1, abs(demultiplexed1) );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023d37a-89a3-4bc6-8212-fcbc2e5b4981",
   "metadata": {},
   "source": [
    "To get the voice signal back from the FFT, we apply the IFFT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d70774-fd4b-4ee0-8ad1-f47e4eeb15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_demultiplexed1 = np.real(numpy.fft.ifft(demultiplexed1)/16384 )\n",
    "sd.play(voice_demultiplexed1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63897a8-bc94-41ef-9694-b75a519ff80d",
   "metadata": {},
   "source": [
    "We can recover the \"quick brown fox\" signal from either the upper frequency band or the lower frequency band. Let's just zero out everything except the uppper frequency band and then shift that band back to the original frequency range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280cad0f-317c-4285-be37-ad729ce11f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "demultiplexed2 = multiplexed.copy()\n",
    "low_freqs = np.where(freqs1 < 4000) # Note that we are not using abs() here because we zero out\n",
    "                                    # ALL negative frequencies\n",
    "demultiplexed2[low_freqs] = 0\n",
    "\n",
    "# Now shift back to baseband (centered at 0)\n",
    "demultiplexed2 = np.roll(demultiplexed2, -freq_shift)\n",
    "\n",
    "plt.plot(freqs1, abs(demultiplexed2) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398410f-6e41-4204-9724-ed5148d1ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_demultiplexed2 = np.real(numpy.fft.ifft(demultiplexed2)/16384 )\n",
    "sd.play(voice_demultiplexed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4901ac-02cf-4ed5-8214-750b4615f2c7",
   "metadata": {},
   "source": [
    "**Questions 3.5: Do you hear any differences between the demultiplexed signals and the filtered signals?  Why do you think you did/did not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db3724d-bd49-4805-9772-7784c8cef144",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Review\n",
    "\n",
    "* Frequency-division multiplexing is a way to put multiple signals on different, non-overlapping frequency bands\n",
    "* The FFT and IFFT are techniques to transform signals into frequency representations, where filtering out frequencies or shifting frequencies are easily done\n",
    "* Demultiplexing is the process of recovering the individual signals that have been multiplexed together.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
